{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honeycomb Query Evals\n",
    "\n",
    "Evals for for Honeycomb Natural Langaguge Query generator from the [Fine Tuning LLMs](https://maven.com/parlance-labs/fine-tuning) course. Related notebooks from the course can be found at <https://github.com/parlance-labs/ftcourse>.\n",
    "\n",
    "The [queries.csv](queries.csv) dataset contains \\~ 2,300 example queries (along with per-query column schemas generated offline via RAG). There are two scoring methods supported\n",
    "(corresponding to the two @task definitions below):\n",
    "\n",
    "1. validate - score using the validity checker from the course (utils.py)\n",
    "2. critique - score using the critique prompt from the course (critique.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Inspect uses a standard schema for [Datasets](https://ukgovernmentbeis.github.io/inspect_ai/datasets.html), so we'll map the raw data into that schema when reading it (note that \"columns\" are saved as metadata so we can use them for prompt engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.dataset import csv_dataset, FieldSpec\n",
    "\n",
    "dataset = csv_dataset(\n",
    "    csv_file=\"queries.csv\",\n",
    "    sample_fields=FieldSpec(input=\"user_input\", metadata=[\"columns\"]),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver\n",
    "\n",
    "To build the prompt, we'll cfreate a custom [Solver](https://ukgovernmentbeis.github.io/inspect_ai/solvers.html) that merges the user query/prompt and the RAG retreived column list into our prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.solver import solver\n",
    "from inspect_ai.util import resource\n",
    "\n",
    "@solver\n",
    "def prompt_with_schema():\n",
    "\n",
    "    prompt_template = resource(\"prompt.txt\")\n",
    "\n",
    "    async def solve(state, generate):\n",
    "        # build the prompt\n",
    "        state.user_prompt.text = prompt_template.replace(\n",
    "            \"{{prompt}}\", state.user_prompt.text\n",
    "        ).replace(\n",
    "            \"{{columns}}\", state.metadata[\"columns\"]\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    return solve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer\n",
    "\n",
    "To score the model's respones to the prompt, we'll create a custom [Scorer](https://ukgovernmentbeis.github.io/inspect_ai/scorers.html) that calls the `is_valid()` function to determine whether a valid query has been constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.scorer import accuracy, scorer, Score, CORRECT, INCORRECT\n",
    "from utils import is_valid, json_completion\n",
    "\n",
    "@scorer(metrics=[accuracy()])\n",
    "def validate_scorer():\n",
    "\n",
    "    async def score(state, target):\n",
    "       \n",
    "        # check for valid query\n",
    "        query = json_completion(state.output.completion)\n",
    "        if is_valid(query, state.metadata[\"columns\"]):\n",
    "            value=CORRECT\n",
    "        else: \n",
    "            value=INCORRECT\n",
    "       \n",
    "        # return score w/ query that was extracted\n",
    "        return Score(value=value, answer=query)\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `json_completion()` function takes care of some details around extracting JSON from a model completion (e.g. removing sorrounding backtick code block emitted by some models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Task\n",
    "\n",
    "Now we'll put all of this together to create an evaluation task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import eval, task, Task\n",
    "from inspect_ai.solver import system_message, generate\n",
    "\n",
    "@task\n",
    "def validate():\n",
    "    return Task(\n",
    "        dataset=dataset,\n",
    "        plan=[\n",
    "            system_message(\"Honeycomb AI suggests queries based on user input.\"),\n",
    "            prompt_with_schema(),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=validate_scorer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the task using Inspect's `eval()` function (limiting to 100 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(validate, model=\"openai/gpt-4-turbo\", limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critique Task\n",
    "\n",
    "Now we'll create a critique task. For this we'll need a LLM-based scorer that uses a critique template to prompt for whether the generated query is \"good\" or \"bad\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from inspect_ai.model import get_model\n",
    "\n",
    "@scorer(metrics=[accuracy()])\n",
    "def critique_scorer(model = \"anthropic/claude-3-opus-20240229\"):\n",
    "\n",
    "    async def score(state, target):\n",
    "       \n",
    "        # build the critic prompt\n",
    "        query = state.output.completion.strip()\n",
    "        critic_prompt = resource(\"critique.txt\").replace(\n",
    "            \"{{prompt}}\", state.user_prompt.text\n",
    "        ).replace(\n",
    "            \"{{columns}}\", state.metadata[\"columns\"]\n",
    "        ).replace(\n",
    "            \"{{query}}\", query\n",
    "        )\n",
    "       \n",
    "        # run the critique\n",
    "        result = await get_model(model).generate(critic_prompt)\n",
    "        try:\n",
    "            parsed = json.loads(json_completion(result.completion))\n",
    "            value = CORRECT if parsed[\"outcome\"] == \"good\" else INCORRECT\n",
    "            explanation = parsed[\"critique\"]\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            value = INCORRECT\n",
    "            explanation = f\"JSON parsing error:\\n{result.completion}\"\n",
    "        \n",
    "        # return value and explanation (critique text)\n",
    "        return Score(value=value, explanation=explanation)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use this scorer in a critique task definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def critique():\n",
    "    return Task(\n",
    "        dataset=dataset,\n",
    "        plan=[\n",
    "            system_message(\"Honeycomb AI suggests queries based on user input.\"),\n",
    "            prompt_with_schema(),\n",
    "            generate()\n",
    "        ],\n",
    "        scorer=critique_scorer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run the task using `eval()` (limiting to 25 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(critique, model=\"openai/gpt-4-turbo\", limit=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
